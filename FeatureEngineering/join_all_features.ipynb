{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all Config Manager features into one dataframe\n",
    "### Step 1: Load data into pandas dataframes \n",
    "Our team has spent some time in creating features from the tables in the Microsoft Configuration Manager database. Our features are stored in separate sources for each Config Manager table that we created features from. We have two types of data sources of features that we need to join together for our model:\n",
    "1. `.parquet` files in blob storage \n",
    "2. SQL views from our cloud database \n",
    "\n",
    "We have tracked the data source file names and SQL views using a spreadsheet, and will use this list to pull the filenames and data from our cloud database and blob storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dtale\n",
    "import plotly.express as px\n",
    "import pyodbc\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the notebook to display all columns of a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Function Definitions --------------\n",
    "\n",
    "def download_blob(cred, LOCALFILENAME,\n",
    "                  CONTAINERNAME, BLOBNAME):\n",
    "\n",
    "    # Start tracking download time\n",
    "    t1=time.time()\n",
    "\n",
    "    # Instantiate a blob service instance\n",
    "    blob_service_client_instance = BlobServiceClient(account_url=cred['in_the_sun'], credential=cred['fun'])\n",
    "    blob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\n",
    "\n",
    "    # Create the file locally\n",
    "    with open(LOCALFILENAME, \"wb\") as my_blob:\n",
    "        blob_data = blob_client_instance.download_blob()\n",
    "        blob_data.readinto(my_blob)\n",
    "\n",
    "    # Notify user of download time\n",
    "    t2=time.time()\n",
    "    print((\"It takes %s seconds to download \"+BLOBNAME) % (t2 - t1))\n",
    "\n",
    "def connect_to_database(cred):\n",
    "    '''\n",
    "    Uses pyodbc to connect to a SQL Server database\n",
    "    and returns the connection object\n",
    "    '''\n",
    "    # Define database driver, server, database\n",
    "    driver = 'SQL SERVER'\n",
    "\n",
    "    # Define the connection string\n",
    "    conn_str = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={cred['server']};DATABASE={cred['db']};UID={cred['uid']};PWD={cred['pwd']}\"\n",
    "\n",
    "    # Connect to database using windows authentication\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "\n",
    "    return conn\n",
    "\n",
    "def get_sql_data(persist_conn_obj, sql_query):\n",
    "    '''\n",
    "    Gets rows from a table in the SQL database connection\n",
    "    '''\n",
    "    # Execute a query for table table\n",
    "    df_pers_all = pd.read_sql_query(sql_query, persist_conn_obj)\n",
    "\n",
    "    return df_pers_all\n",
    "\n",
    "def get_creds(in_file):\n",
    "    with open(in_file) as f:\n",
    "        cred = json.load(f)\n",
    "    return cred\n",
    "\n",
    "# Get creds\n",
    "creds_file = 'assets/fun.json'\n",
    "creds = get_creds(creds_file)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_metadata():\n",
    "\n",
    "    # Read the EDA feature metadata \n",
    "    df = pd.read_excel('assets/EDA _ Feature Engineering Tasks.xlsx', sheet_name='Features to Engineer')\n",
    "\n",
    "    # Select only completed features \n",
    "    df['Completed? (Y/N)'] = df['Completed? (Y/N)'].apply(lambda x: x.strip())\n",
    "    df = df[df['Completed? (Y/N)'] == 'Y']\n",
    "\n",
    "    # Get only one row per view/filename\n",
    "    out_df = df[['Completed dataset location', 'View/Filename', 'Keys']]\n",
    "    out_df = out_df.drop_duplicates()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "def download_all_data(metadata_df, save_directory, creds):\n",
    "\n",
    "    # Instantiate a dictionary to accumulate dataframes\n",
    "    feature_dfs = {}\n",
    "\n",
    "    # Start a SQL connection \n",
    "    conn = connect_to_database(creds)\n",
    "\n",
    "    # Iterate through each view/file\n",
    "    for row in metadata_df.iterrows():\n",
    "        \n",
    "        row = row[1]\n",
    "\n",
    "        # Download and read Blob file\n",
    "        if row['Completed dataset location'] == 'Blob Storage':\n",
    "            \n",
    "            # Download from blob storage\n",
    "            blob_name = row['View/Filename']\n",
    "            file_save_name = os.path.join(save_directory, blob_name)\n",
    "            download_blob(creds, file_save_name, 'configmanagertest1', blob_name)\n",
    "\n",
    "            # Read the blob into a dataframe and add to output dict\n",
    "            current_df = pd.read_parquet(file_save_name)\n",
    "            feature_dfs[blob_name] = current_df\n",
    "\n",
    "        # Download and read SQL \n",
    "        if row['Completed dataset location'] == 'SQL View':\n",
    "            \n",
    "            # Call SQL to get data\n",
    "            view_name = row['View/Filename']\n",
    "            sql_query = f'SELECT * FROM {view_name}'\n",
    "            current_df = get_sql_data(conn, sql_query)\n",
    "\n",
    "            # Add data to output dict\n",
    "            feature_dfs[view_name] = current_df\n",
    "            print(f'retrieved {view_name} from SQL')\n",
    "    \n",
    "    # Close the SQL connection\n",
    "    conn.close()\n",
    "\n",
    "    return feature_dfs\n",
    "\n",
    "# Download data \n",
    "save_location = 'assets/features'\n",
    "df = get_feature_metadata()\n",
    "feature_datasets = download_all_data(df, save_location, creds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to yield a generator of dataframes \n",
    "def view_data(in_dict):\n",
    "\n",
    "    for key in in_dict.keys():\n",
    "        \n",
    "        dat = in_dict[key]\n",
    "        if 'RWB_EFFECTIVE_DATE' in dat.columns:\n",
    "            dat['RWB_EFFECTIVE_DATE'] = pd.to_datetime(dat['RWB_EFFECTIVE_DATE'])\n",
    "            min_date = dat['RWB_EFFECTIVE_DATE'].min()\n",
    "            max_date = dat['RWB_EFFECTIVE_DATE'].max()\n",
    "            print(key, f\"MIN_DATE: {min_date}\", f\"MAX_DATE: {max_date}\" )\n",
    "        yield dat\n",
    "\n",
    "generator = view_data(feature_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View each dataframe \n",
    "# current = next(generator)\n",
    "# current.head()\n",
    "for val in generator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_grouped = feature_datasets['num_events.pq']\n",
    "events_grouped['events'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Join all features into one dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_all_features(features_dict, metadata_df):\n",
    "\n",
    "    # Get the system_disc table \n",
    "    system_name = 'system_disc_features.pq'\n",
    "    system_df = features_dict[system_name]\n",
    "\n",
    "    # Drop rows in system_disc that have may 20th 2023\n",
    "    system_df = system_df[system_df['RWB_EFFECTIVE_DATE'] != '2023-05-20']\n",
    "\n",
    "    # Update the type of RWB_EFFECTIVE_DATE to str\n",
    "    system_df['RWB_EFFECTIVE_DATE'] = system_df['RWB_EFFECTIVE_DATE'].astype(str)\n",
    "\n",
    "    # Iterate through each file \n",
    "    for row in metadata_df.iterrows():\n",
    "        row = row[1]\n",
    "\n",
    "        # Get the table and key on the right to join to system disc\n",
    "        right_df_name = row['View/Filename']\n",
    "        right_df = features_dict[right_df_name]\n",
    "        right_keys = row['Keys'].split(', ')\n",
    "\n",
    "        if right_df_name == 'num_events_inc.pq':\n",
    "            right_df = right_df.rename(columns={\"events\":\"impactful_events\"})\n",
    "        if right_df_name == 'num_events_office.pq':\n",
    "            right_df = right_df.rename(columns={\"events\":\"office_events\"})\n",
    "\n",
    "        # skip if df is system disc\n",
    "        if right_df_name == system_name:\n",
    "            continue\n",
    "        \n",
    "        # Get the system disc key and change type of right key date\n",
    "        if 'RWB_EFFECTIVE_DATE' in right_keys:\n",
    "            left_keys = ['ItemKey', 'RWB_EFFECTIVE_DATE']\n",
    "            right_df['RWB_EFFECTIVE_DATE'] = right_df['RWB_EFFECTIVE_DATE'].astype(str)\n",
    "            right_df = right_df[right_df['RWB_EFFECTIVE_DATE'] != '2023-05-20']\n",
    "        elif 'created_date' in right_keys:\n",
    "            left_keys = ['ItemKey', 'RWB_EFFECTIVE_DATE']\n",
    "            right_df['created_date'] = right_df['created_date'].astype(str)\n",
    "        elif 'CreatedSystemTime_CST_formatted' in right_keys:\n",
    "            right_df['CreatedSystemTime_CST_formatted'] = right_df['CreatedSystemTime_CST_formatted'].astype(str)\n",
    "            right_df['ClientItemKey'] = right_df['ClientItemKey'].astype(int)\n",
    "        else:\n",
    "            left_keys = ['ItemKey']\n",
    "\n",
    "        # Perform a left join for the features \n",
    "        system_df = pd.merge(system_df, right_df, \n",
    "                             how='left',\n",
    "                             left_on=left_keys,\n",
    "                             right_on=right_keys,\n",
    "                             copy=False)\n",
    "        \n",
    "        if 'MachineID' in system_df.columns:\n",
    "            system_df = system_df.drop('MachineID', axis=1)\n",
    "        elif 'ClientItemKey' in system_df.columns:\n",
    "            system_df = system_df.drop('ClientItemKey', axis=1)\n",
    "        elif 'created_date' in system_df.columns:\n",
    "            system_df = system_df.drop('created_date', axis=1)\n",
    "        elif 'CreatedSystemTime_CST_formatted' in system_df.columns:\n",
    "            system_df = system_df.drop('CreatedSystemTime_CST_formatted', axis=1)\n",
    "        \n",
    "    return system_df\n",
    "\n",
    "final_df = join_all_features(feature_datasets, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View MissingNo matrix\n",
    "msno.matrix(final_df, labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look into nullness in update events\n",
    "update_df = feature_datasets['update_summary_features.parquet']\n",
    "print('number of unique machines in Update Events: ', len(update_df['ClientItemKey'].unique()))\n",
    "print('number of unique machines in System DISC: ', len(final_df['ItemKey'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look into nullness in BOOT and App Crash events\n",
    "boot_df = feature_datasets['boot_events_parsed_features.pq']\n",
    "events_df = feature_datasets['num_events.pq']\n",
    "print('number of unique machines in BOOT Events: ', len(boot_df['ClientItemKey'].unique()))\n",
    "print('number of unique machines in APP CRASH/HANG Events: ', len(events_df['ClientItemKey'].unique()))\n",
    "print('number of unique machines in System DISC: ', len(final_df['ItemKey'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_nullness(in_df):\n",
    "\n",
    "    out_dict = {}\n",
    "    out_dict['Attribute'] = []\n",
    "    out_dict['Not_Null_Count'] = []\n",
    "\n",
    "    for col in in_df.columns:\n",
    "\n",
    "        # Grab number of non-null records for \n",
    "        # each attribute\n",
    "        out_dict['Attribute'].append(col)\n",
    "        out_dict['Not_Null_Count'].append(len(in_df[in_df[col].notnull()]))\n",
    "\n",
    "    out_df = pd.DataFrame(out_dict)\n",
    "    out_df = out_df.sort_values('Not_Null_Count', ascending=True)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "null_summary_df = investigate_nullness(final_df)\n",
    "null_summary_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "* Attributes that were created from software inventory tables do not have as much coverage as attributes that were created from the hardware inventory views.\n",
    "* It appears that update events are not captured for all machines. However, a total of 7894 machines in the update events dataset does sound correct for the number of laptops and desktops in the organization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature cleaning notes \n",
    "\n",
    "### High null counts \n",
    "* Fill null values for update events with zero\n",
    "* Drop TotalBootDurationInMilliseconds \n",
    "* Fill Missing values for boolean battery features with 0, assuming that if there are no battery data that the machine does not have a battery\n",
    "* For EstimatedRunTime00, fill missing values with extreme values that occur outside of the range of given values, assuming that the machine is always plugged in and will run infinitely\n",
    "* Fill boolean addin features with zero\n",
    "* Fill boolean programs features with zero\n",
    "\n",
    "### Removing date features \n",
    "We need to remove the following date features so our model only contains numeric values:\n",
    "\n",
    "`['created_date', 'InstallDate00', 'LastBootUpTime00', 'BIOS Release Date', 'LastHWScan']`\n",
    "\n",
    "### Drop rows that have any `NaN` value\n",
    "Given time constraints and dataset size, we will be dropping any rows that have a `NaN` missing value. Future work may attempt to impute the missing values. We will save the records that were dropped to do analysis and check for patterns in machine characteristics to make sure we are not losing a large subset of machine categories.\n",
    "\n",
    "### Step 3: Clean the joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nan_columns(in_features, in_df):\n",
    "\n",
    "    # Function for filling NaNs with zero\n",
    "    fill_col_nan = lambda col: in_df[col].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "\n",
    "    # Call fill na function \n",
    "    for feature in in_features:\n",
    "        \n",
    "        # Fill missing estimated run time with extreme values\n",
    "        if feature == 'EstimatedRunTime00':\n",
    "            max_val = in_df[feature].max() * 100\n",
    "            in_df[feature] = in_df[feature].apply(lambda x: max_val if pd.isna(x) else x)\n",
    "        else:\n",
    "            in_df[feature] = fill_col_nan(feature)\n",
    "\n",
    "    return in_df\n",
    "\n",
    "def clean_joined_data(in_df, return_impactful=False, return_office=False):\n",
    "\n",
    "    # fill missing values with zero for update columns\n",
    "    update_features = ['num_updates', 'num_windows_64_os_updates','num_office_updates']\n",
    "    \n",
    "    # fill missing values with zero for addin and program columns\n",
    "    boolean_features_addins_programs = ['has_powerbi', 'num_installed_programs',\n",
    "       'num_x64addins', 'num_x86addins', 'x64add_avgloadtime',\n",
    "       'x86add_avgloadtime', 'has_cap_iq_add', 'has_factset_add',\n",
    "       'has_bluematrix_add', 'has_bloomberg_add', 'has_acrobat_add',\n",
    "       'Accessx86_addin_filesize', 'Excelx64_addin_filesize',\n",
    "       'Excelx86_addin_filesize', 'OneNotex64_addin_filesize',\n",
    "       'OneNotex86_addin_filesize', 'Outlookx64_addin_filesize',\n",
    "       'Outlookx86_addin_filesize', 'PowerPointx64_addin_filesize',\n",
    "       'PowerPointx86_addin_filesize', 'Publisherx86_addin_filesize',\n",
    "       'Wordx64_addin_filesize', 'Wordx86_addin_filesize']\n",
    "    \n",
    "    # Define battery features \n",
    "    battery_features = ['Battery_Power', 'AC_Power', 'EstimatedRunTime00']\n",
    "\n",
    "    features_to_fill = update_features + boolean_features_addins_programs + battery_features\n",
    "    in_df = handle_nan_columns(features_to_fill, in_df)\n",
    "    \n",
    "    # drop Total Boot duration in Milliseconds and remove date features\n",
    "    high_nulls = ['TotalBootDurationInMilliseconds', 'CreatedSystemTime_CST_formatted']\n",
    "    date_features = ['created_date', 'InstallDate00', 'LastBootUpTime00',\n",
    "                      'BIOS Release Date', 'LastHWScan', 'CreatedSystemTime_CST_formatted_y',\n",
    "                      'CreatedSystemTime_CST_formatted_x', 'CreatedSystemTime_CST_formatted_y']\n",
    "    cols_to_drop = high_nulls + date_features\n",
    "    out_df = in_df[[col for col in in_df.columns if col not in cols_to_drop]]\n",
    "\n",
    "    # If event data is NaN, let's assume the machine did not \n",
    "    # have an event for that day.\n",
    "    if return_impactful:\n",
    "        # Assign events variable the number of impactful events\n",
    "        out_df['events'] = out_df['impactful_events'].apply(lambda x: x if pd.notnull(x) else 0)\n",
    "    \n",
    "    if return_office:\n",
    "        # Assign events variable the number of office events\n",
    "        out_df['events'] = out_df['office_events'].apply(lambda x: x if pd.notnull(x) else 0)\n",
    "\n",
    "    else:\n",
    "        out_df['events'] = out_df['events'].apply(lambda x: x if pd.notnull(x) else 0)\n",
    "\n",
    "    # Drop the impactful and office events attribute \n",
    "    out_df = out_df.drop('impactful_events', axis=1)\n",
    "    out_df = out_df.drop('office_events', axis=1)\n",
    "\n",
    "    # Drop rows with any NaN value\n",
    "    out_df = out_df.dropna(axis=0, how='any')\n",
    "\n",
    "    # For impactful events, keep only 10,000 records where the number of events is zero\n",
    "    if return_impactful:\n",
    "        zero_df = out_df[out_df['events'] == 0]\n",
    "        nonzero_df = out_df[out_df['events'] > 0]\n",
    "        zero_df = zero_df.sample(10000, random_state=42)\n",
    "        out_df = pd.concat([zero_df, nonzero_df], axis=0)\n",
    "\n",
    "    if return_office:\n",
    "        zero_df = out_df[out_df['events'] == 0]\n",
    "        nonzero_df = out_df[out_df['events'] > 0]\n",
    "        zero_df = zero_df.sample(40000, random_state=42)\n",
    "        out_df = pd.concat([zero_df, nonzero_df], axis=0)\n",
    "\n",
    "    # Drop columns that have only one value\n",
    "    for col in out_df.columns:\n",
    "        if len(out_df[col].unique()) <= 1:\n",
    "            print(col)\n",
    "            out_df = out_df.drop(col, axis=1)\n",
    "\n",
    "    return out_df\n",
    "\n",
    "cleaned_df = clean_joined_data(final_df, return_impactful=False, return_office=True)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "* Machines with `system_type` that are `x86_64` and `arm64` are dropped from the data when we drop rows with any `NaN` values.\n",
    "* Only 10% of the final joined dataset remains after dropping rows with any `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('final_df length:', len(final_df))\n",
    "print('cleaned_df length:', len(cleaned_df))\n",
    "#181352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models that were dropped\n",
    "print(cleaned_df['Model'].unique())\n",
    "print(final_df['Model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, view data in Dtale\n",
    "dtale.show(cleaned_df).open_browser() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(cleaned_df, x=\"events\", log_y=True, title=\"Histogram of number of events per machine per day in log scale\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_attr(in_df, attrs):\n",
    "    \n",
    "    # Get the category labels and  categories\n",
    "    categories = [(attr, list(in_df[attr].unique())) for attr in attrs]\n",
    "    ohe_columns = [x[0] for x in categories]\n",
    "    ohe_categories = [x[1] for x in categories]\n",
    "\n",
    "    # Instantiate the one hot encoder and fit it \n",
    "    enc = OneHotEncoder(sparse_output=False, categories=ohe_categories)\n",
    "    transformer = make_column_transformer((enc, ohe_columns), remainder='passthrough')\n",
    "    output = transformer.fit_transform(in_df)\n",
    "\n",
    "    # Put results in dataframe and clean up columns\n",
    "    out_df = pd.DataFrame(output, columns=transformer.get_feature_names_out())\n",
    "    remainder_cols =  [col[11:] for col in out_df.columns if 'remainder' in col]\n",
    "    encoded_cols = [col[15:] for col in out_df.columns if 'onehotencoder' in col]\n",
    "    out_df.columns = encoded_cols + remainder_cols\n",
    "    out_df = out_df[remainder_cols + encoded_cols]\n",
    "\n",
    "    return out_df\n",
    "\n",
    "encoded_clean_df = encode_attr(cleaned_df, ['Model', 'CaseType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Split data into train, test, validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_modelling(df):\n",
    "\n",
    "    # Assuming your DataFrame is named 'df'\n",
    "    # Splitting the data into train and temp sets (80% train + 20% temp)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Splitting the temp set into test and validation sets (50% test + 50% validation)\n",
    "    test_df, validation_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Printing the shapes of the resulting sets\n",
    "    print(\"Train set shape:\", train_df.shape)\n",
    "    print(\"Test set shape:\", test_df.shape)\n",
    "    print(\"Validation set shape:\", validation_df.shape)\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = split_data_modelling(encoded_clean_df)\n",
    "\n",
    "# View shape of label distribution for training data in log scale \n",
    "fig = px.histogram(train_df, x=\"events\", log_y=True, title=\"Histogram of number of events per machine per day from training data in log scale\")\n",
    "fig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: undersample majority class from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_majority_labels(in_df, target_var, rare_boundary_threshold, random_state=42):\n",
    "    \n",
    "    # find the label at the boundary of label rarity\n",
    "    rarity_boundary = np.percentile(in_df[target_var], rare_boundary_threshold)\n",
    "    print(rarity_boundary)\n",
    "\n",
    "    # Get the number of records in the minority class\n",
    "    minority_df =in_df[in_df[target_var] > rarity_boundary]\n",
    "    num_minority = len(minority_df)\n",
    "\n",
    "    # get the labels in the majority \n",
    "    majority_df = in_df[in_df[target_var] <= rarity_boundary]\n",
    "    maj_labs = majority_df[target_var].unique()\n",
    "\n",
    "    # get the size of partitions for the majority labels\n",
    "    partition_size = num_minority//len(maj_labs)\n",
    "\n",
    "    # copy the minority df for the output\n",
    "    output_df = minority_df.copy()\n",
    "\n",
    "    for maj_label in maj_labs:\n",
    "        \n",
    "        # get the data for the majority label\n",
    "        maj_df = in_df[in_df[target_var] ==  maj_label]\n",
    "\n",
    "        # Randomly sample the data \n",
    "        maj_sample = maj_df.sample(partition_size, random_state=random_state)\n",
    "\n",
    "        # add to output dataframe\n",
    "        output_df = pd.concat([output_df, maj_sample], axis=0)\n",
    "\n",
    "    print('shape of undersampled dataframe: ', output_df.shape)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "train_undersamp_df = undersample_majority_labels(train_df, 'events', 85)\n",
    "\n",
    "# Show the new training data label distribution \n",
    "fig = px.histogram(train_undersamp_df, x=\"events\", log_y=True, title=\"Histogram of number of events per machine per day from undersampled training data in log scale\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export train, test, and validation sets to dataframe \n",
    "train_name = 'assets/training_set_office.parquet'\n",
    "test_name = 'assets/test_set_impactful_office.parquet'\n",
    "validate_name = 'assets/validation_set_office.parquet'\n",
    "\n",
    "# Save datasets to parquet\n",
    "train_undersamp_df.to_parquet(train_name)\n",
    "test_df.to_parquet(test_name)\n",
    "validation_df.to_parquet(validate_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
