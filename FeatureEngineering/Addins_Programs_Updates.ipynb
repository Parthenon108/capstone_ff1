{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Addins, Programs, and Updates\n",
    "This notebook is intended to engineer the features from `OFFICE_ADDIN_DATA`, `Add_Remove_Programs`, and update events from `EventRawResultItem`.\n",
    "\n",
    "### Engineer update event features\n",
    "Here I will create 3 features:\n",
    "1.\tNumber of updates installed \n",
    "2.\tNumber of Windows 10 updates installed\n",
    "3.\tNumber of office updates installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import dtale\n",
    "\n",
    "# Set the notebook to display all columns of a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def handle_none_values(in_val):\n",
    "\n",
    "    if type(in_val) == type(None) or in_val == 'None':\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return in_val\n",
    "    \n",
    "def read_json_fill_attr(jsonfile, in_df, attr):\n",
    "\n",
    "    attr_df = pd.read_json(jsonfile, orient='index')\n",
    "    in_dict = attr_df.reset_index().set_index('id').to_dict(orient='index')\n",
    "    in_df[attr] = in_df[attr].apply(lambda x: in_dict[x]['index'])\n",
    "\n",
    "    return in_df\n",
    "\n",
    "def read_update_data(infile):\n",
    "\n",
    "    # Read in INC and category df\n",
    "    inc_df = pd.read_parquet(infile)\n",
    "\n",
    "    # Filter out erroneous Nones in the data \n",
    "    for col in inc_df.columns:\n",
    "        inc_df[col] = inc_df[col].apply(lambda x: handle_none_values(x))\n",
    "\n",
    "    out_inc = read_json_fill_attr('assets/updateTitle.json', inc_df, 'updateTitle')\n",
    "\n",
    "    return out_inc\n",
    "\n",
    "# Get incident data \n",
    "#df = read_update_data('assets/update_events.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_counts(in_dat):\n",
    "\n",
    "    output_dict = {}\n",
    "\n",
    "    # Get total number of updates\n",
    "    output_dict['num_updates'] = len(in_dat)\n",
    "\n",
    "    # Get the number of windows os updates\n",
    "    output_dict['num_windows_64_os_updates'] = in_dat['win_64_os_update'].sum()\n",
    "\n",
    "    # Get the number of office updates\n",
    "    output_dict['num_office_updates'] = in_dat['office_update'].sum()\n",
    "\n",
    "    # Return a series for the group \n",
    "    out_series = pd.Series(output_dict, index=list(output_dict.keys()))\n",
    "\n",
    "    return out_series\n",
    " \n",
    "def get_update_features(in_df):\n",
    "\n",
    "    # lower case for updateTitle \n",
    "    in_df['updateTitle'] = in_df['updateTitle'].str.lower()\n",
    "\n",
    "    # Change created system time to type datetime\n",
    "    in_df['TimeCreatedSystemTime'] = pd.to_datetime(in_df['TimeCreatedSystemTime'])\n",
    "\n",
    "    # Add created date \n",
    "    in_df['created_date'] = in_df['TimeCreatedSystemTime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create identifier for x64 based Windows OS updates \n",
    "    in_df['win_64_os_update'] = in_df['updateTitle'].apply(lambda x: 1 if ('cumulative update for' in x) or ('windows 10' in x) else 0)\n",
    "\n",
    "    # Create identifier for office updates\n",
    "    in_df['office_update'] = in_df['updateTitle'].apply(lambda x: 1 if 'office' in x else 0)\n",
    "\n",
    "    # Group by and get results \n",
    "    out_gb = in_df.groupby(['ClientItemKey', 'created_date']).apply(get_update_counts).reset_index()\n",
    "\n",
    "    return in_df, out_gb\n",
    "\n",
    "# Get features\n",
    "# processd_df, grouped_df = get_update_features(df)\n",
    "\n",
    "# Confirm results are expected\n",
    "# dtale.show(grouped_df).open_browser()\n",
    "\n",
    "# Save result to parquet\n",
    "# grouped_df.to_parquet('assets/update_summary_features.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer Addin Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_addin_data(addin_data_directory):\n",
    "\n",
    "    # Get the files in the directory\n",
    "    files = os.listdir(addin_data_directory)\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "\n",
    "            filepath = os.path.join(addin_data_directory, file)\n",
    "            \n",
    "            # Read in the parquet file\n",
    "            chunk = pd.read_parquet(filepath, engine='pyarrow')\n",
    "\n",
    "            # Drop row version \n",
    "            chunk = chunk.drop('rowversion', axis=1)\n",
    "\n",
    "            # Fill friendly name, product name, and company name with values\n",
    "            friendlyname = os.path.join(addin_data_directory, 'FriendlyName.json')\n",
    "            companyname = os.path.join(addin_data_directory, 'CompanyName.json')\n",
    "            productname = os.path.join(addin_data_directory, 'ProductName.json')\n",
    "            chunk = read_json_fill_attr(friendlyname, chunk, 'FriendlyName00')\n",
    "            chunk = read_json_fill_attr(companyname, chunk, 'CompanyName00')\n",
    "            chunk = read_json_fill_attr(productname, chunk, 'ProductName00')\n",
    "\n",
    "            # Append result to output\n",
    "            out_df = pd.concat([out_df, chunk], axis=0)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "# Profile first chunk\n",
    "# df = read_addin_data('assets/office_addin_data')\n",
    "# addin_df = next(dfs)\n",
    "\n",
    "# Optional, view result in dtale\n",
    "# dtale.show(addin_df).open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def identify_addin(in_dat, attr_name, re_pat):\n",
    "\n",
    "    in_dat[attr_name] = in_dat['FriendlyName00'].apply(lambda x: 1 if re.search(re_pat, x) else 0)\n",
    "\n",
    "    return in_dat\n",
    "\n",
    "\n",
    "def get_specific_addin_presence(group, addin_attrs):\n",
    "\n",
    "    out_dict = {}\n",
    "\n",
    "    # Get attribute presence for the group\n",
    "    for attribute in addin_attrs:\n",
    "\n",
    "        if group[attribute].sum() > 0:\n",
    "            out_dict[attribute] = 1\n",
    "        else: \n",
    "            out_dict[attribute] = 0\n",
    "    \n",
    "    # Return results in the series\n",
    "    out_series = pd.Series(out_dict, index=list(out_dict.keys()))\n",
    "\n",
    "    return out_series\n",
    "\n",
    "def get_addins_features(in_dat):\n",
    "\n",
    "    # Function to use in the group by\n",
    "    get_num_addins = lambda x: pd.Series({\"num_addins\": len(x['Id00'].unique()),\n",
    "                                            \"avg_loadtime\": x['AverageLoadTimeInMilliseconds00'].mean()}, \n",
    "                                         index=[\"num_addins\", \"avg_loadtime\"])\n",
    "\n",
    "    # Get architecture-specific addin features\n",
    "    gb_fields = ['MachineID', 'RWB_EFFECTIVE_DATE', 'Architecture00']\n",
    "    out_gb = in_dat.groupby(gb_fields).apply(get_num_addins)\n",
    "    out_gb = out_gb.reset_index()\n",
    "\n",
    "    # Pivot results \n",
    "    out_gb['Architecture00'] = 'num_' + out_gb['Architecture00'] + '_addins'\n",
    "    out_gb = out_gb.pivot(index=['MachineID', 'RWB_EFFECTIVE_DATE'],\n",
    "                          columns='Architecture00',\n",
    "                          values = ['num_addins', 'avg_loadtime']\n",
    "                          )\n",
    "    \n",
    "    # Clean results\n",
    "    out_gb =out_gb.reset_index()\n",
    "    out_gb.columns = ['MachineID', 'RWB_EFFECTIVE_DATE', 'num_x64addins',\n",
    "                      'num_x86addins', 'x64add_avgloadtime', 'x86add_avgloadtime']\n",
    "    \n",
    "    # Define addins to check presence\n",
    "    addin_attrs = ['has_cap_iq_add', 'has_factset_add', 'has_bluematrix_add',\n",
    "                'has_bloomberg_add', 'has_acrobat_add']\n",
    "\n",
    "    # Get specific addin presence features\n",
    "    gb_fields2 = ['MachineID', 'RWB_EFFECTIVE_DATE']\n",
    "    out_gb2 = in_dat.groupby(gb_fields2).apply(get_specific_addin_presence, \n",
    "                                               addin_attrs=addin_attrs)\n",
    "    out_gb2 = out_gb2.reset_index()\n",
    "\n",
    "    # Join addin presence and architecture-specific addin features\n",
    "    output_df = pd.merge(out_gb, out_gb2, \n",
    "                         how='outer', \n",
    "                         left_on=['MachineID', 'RWB_EFFECTIVE_DATE'],\n",
    "                         right_on=['MachineID', 'RWB_EFFECTIVE_DATE'])\n",
    "\n",
    "    return output_df\n",
    "\n",
    "def get_addin_filesize(in_dat):\n",
    "\n",
    "    # Function to use in the group by\n",
    "    get_total_filesize = lambda x: pd.Series({\"total_filesize\": x['FileSize00'].sum()}, \n",
    "                                         index=[\"total_filesize\"])\n",
    "    \n",
    "    # Group by Machine, day and architecture to get results\n",
    "    gb_fields = ['MachineID', 'RWB_EFFECTIVE_DATE', 'OfficeApp00', 'Architecture00']\n",
    "    out_gb = in_dat.groupby(gb_fields).apply(get_total_filesize)\n",
    "    out_gb = out_gb.reset_index()\n",
    "\n",
    "    # Pivot results \n",
    "    out_gb['summary_vals'] =  out_gb['OfficeApp00'] + out_gb['Architecture00'] + '_addin_filesize'\n",
    "    out_gb = out_gb.pivot(index=['MachineID', 'RWB_EFFECTIVE_DATE'],\n",
    "                          columns='summary_vals',\n",
    "                          values = 'total_filesize'\n",
    "                          )\n",
    "    out_gb = out_gb.reset_index()\n",
    "\n",
    "    return out_gb\n",
    "\n",
    "def create_addin_features(in_df):\n",
    "\n",
    "    # Change effective date to type date\n",
    "    in_df['RWB_EFFECTIVE_DATE'] = pd.to_datetime(in_df['RWB_EFFECTIVE_DATE']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create identifier for CAP IQ \n",
    "    cap_pat = 'Cap IQ|Capital IQ|cap iq|capital iq'\n",
    "    in_df = identify_addin(in_df, 'has_cap_iq_add', cap_pat)\n",
    "\n",
    "    # Create identifier for FactSet \n",
    "    cap_pat = 'FactSet|factset'\n",
    "    in_df = identify_addin(in_df, 'has_factset_add', cap_pat)\n",
    "\n",
    "    # Create identifier for BlueMatrix, Bloomberg, and acrobat\n",
    "    in_df['has_bluematrix_add'] = in_df['CompanyName00'].apply(lambda x: 1 if x == 'BlueMatrix I LLC' else 0)\n",
    "    in_df['has_bloomberg_add'] = in_df['CompanyName00'].apply(lambda x: 1 if x == 'Bloomberg LP' else 0)\n",
    "    in_df['has_acrobat_add'] = in_df['CompanyName00'].apply(lambda x: 1 if x == 'Adobe Systems Incorporated' else 0)\n",
    "\n",
    "    # Get the number of 64-bit and 32-bit Office addins \n",
    "    out_gb = get_addins_features(in_df)\n",
    "\n",
    "    # Get file size of addins per office app \n",
    "    out_gb2 = get_addin_filesize(in_df)\n",
    "\n",
    "    # Join all addin features\n",
    "    output_df = pd.merge(out_gb, out_gb2, \n",
    "                         how='outer', \n",
    "                         left_on=['MachineID', 'RWB_EFFECTIVE_DATE'],\n",
    "                         right_on=['MachineID', 'RWB_EFFECTIVE_DATE'])\n",
    "    \n",
    "    # Replace NaNs with zeros for machines that do not have addins\n",
    "    output_df = output_df.replace(np.nan, 0)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "# grouped_df = create_addin_features(df)\n",
    "\n",
    "# Optional, show results of processed dataframe\n",
    "# dtale.show(processed_df).open_browser()\n",
    "\n",
    "# Send results to parquet\n",
    "# grouped_df.to_parquet('assets/office_addin_features.parquet', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer Program Features\n",
    "I'm going to engineer the following features from add_remove_programs:\n",
    "1. Average software age\n",
    "2. Number of installed programs\n",
    "3. Power BI desktop installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_exceptions(in_val, indict):\n",
    "\n",
    "    try:\n",
    "        output = indict[in_val]['index']\n",
    "\n",
    "    except:\n",
    "        output = in_val\n",
    "    \n",
    "    return output\n",
    "\n",
    "def read_json_fill_attr(jsonfile, in_df, attr):\n",
    "\n",
    "    attr_df = pd.read_json(jsonfile, orient='index')\n",
    "    in_dict = attr_df.reset_index().set_index('id').to_dict(orient='index')\n",
    "    in_df[attr] = in_df[attr].apply(lambda x: handle_exceptions(x, in_dict))\n",
    "\n",
    "    return in_df\n",
    "\n",
    "def read_programs_data(data_directory):\n",
    "\n",
    "    # Get the files in the directory\n",
    "    files = os.listdir(data_directory)\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "\n",
    "            filepath = os.path.join(data_directory, file)\n",
    "            \n",
    "            # Read in the parquet file\n",
    "            chunk = pd.read_parquet(filepath, engine='pyarrow')\n",
    "\n",
    "            # Drop unneeded columns \n",
    "            read_columns = ['MachineID', 'RWB_EFFECTIVE_DATE', 'ProdID00', \n",
    "                            'DisplayName00', 'InstallDate00' ]\n",
    "            chunk = chunk[read_columns]\n",
    "\n",
    "            # Fill friendly name, product name, and company name with values\n",
    "            displayname = os.path.join(data_directory, 'DisplayName00.json')\n",
    "            prodid = os.path.join(data_directory, 'ProdID00.json')\n",
    "            chunk = read_json_fill_attr(displayname, chunk, 'DisplayName00')\n",
    "            chunk = read_json_fill_attr(prodid, chunk, 'ProdID00')\n",
    "\n",
    "            # Append result to output\n",
    "            out_df = pd.concat([out_df, chunk], axis=0)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "df = read_programs_data('assets/add_remove_programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_group_data(group):\n",
    "\n",
    "    out_dict = {}\n",
    "\n",
    "    # Get attribute presence for the g\n",
    "\n",
    "    if group['has_powerbi'].sum() > 0:\n",
    "        out_dict['has_powerbi'] = 1\n",
    "    else: \n",
    "        out_dict['has_powerbi'] = 0\n",
    "    \n",
    "    out_dict['num_installed_programs'] = len(group['ProdID00'].unique())\n",
    "    out_dict['avg_software_age'] = group['SoftwareAge'].mean()\n",
    "    \n",
    "    # Return results in the series\n",
    "    out_series = pd.Series(out_dict, index=list(out_dict.keys()))\n",
    "\n",
    "    return out_series\n",
    "\n",
    "def parse_dates(val):\n",
    "\n",
    "    if pd.notnull(val):\n",
    "        if (len(val) == 8) or ('/' in val):\n",
    "            return pd.to_datetime(val, format='mixed').strftime('%Y-%m-%d')\n",
    "    \n",
    "    else: \n",
    "        return np.nan\n",
    "\n",
    "def calculate_age_in_days(target_date_str, in_date):\n",
    "   \n",
    "   # Convert target_date_str to a Python datetime object\n",
    "   target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()\n",
    "\n",
    "   # Calculate the age in days\n",
    "   age_in_days = (in_date - target_date).days\n",
    "\n",
    "   return age_in_days\n",
    "\n",
    "def create_program_features(in_df):\n",
    "\n",
    "    # Change effective date to type date\n",
    "    in_df['RWB_EFFECTIVE_DATE'] = pd.to_datetime(in_df['RWB_EFFECTIVE_DATE']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Change install date values to type datetime\n",
    "    in_df['InstallDate00'] = in_df['InstallDate00'].apply(lambda x: parse_dates(x))\n",
    "\n",
    "    # Get age of software in days\n",
    "    todaydate = date(2023, 7, 4)\n",
    "    in_df['SoftwareAge'] = in_df['InstallDate00'].apply(lambda x: calculate_age_in_days(x, todaydate)\n",
    "                                                        if pd.notnull(x) else np.nan)\n",
    "\n",
    "    # Create identifier for CAP IQ \n",
    "    power_biregex = 'Power BI Desktop|Microsoft Power BI Report Server|Microsoft PowerBI Desktop'\n",
    "    in_df['DisplayName00'] = in_df['DisplayName00'].astype(str)\n",
    "    in_df['has_powerbi'] = in_df['DisplayName00'].apply(lambda x: 1 if re.search(power_biregex, x) else 0)\n",
    "\n",
    "    # Get the main groupby features\n",
    "    out_gb = in_df.groupby(['MachineID', 'RWB_EFFECTIVE_DATE']).apply(get_group_data)\n",
    "    out_gb = out_gb.reset_index()\n",
    "\n",
    "    # Handle negative software ages\n",
    "    out_gb['avg_software_age'] = out_gb['avg_software_age'].apply(lambda x: x if x >=0 else np.nan)\n",
    "\n",
    "    return out_gb\n",
    "\n",
    "grouped_df = create_program_features(df)\n",
    "\n",
    "# Optional, show results of processed dataframe\n",
    "# dtale.show(grouped_df).open_browser()\n",
    "\n",
    "# Send results to parquet\n",
    "grouped_df.to_parquet('assets/add_remove_programs_features.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
