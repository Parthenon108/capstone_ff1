{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Set the notebook to display all columns of a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# import service now data \n",
    "inc_df = pd.read_csv(r'do_not_commit\\Datasets\\SerivceNow_Incident.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ivestigate machines with more than one incident logged on the same day\n",
    "We need to be able to join the incidents data with the events data. To do this properly, we will need there to be a many-to-one relationship between events and incidents. Let's evaluate the extent of machines that have more than one incident per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_duplicate_machine_groups(inc_df, return_dups_only=True):\n",
    "\n",
    "    # Reformat date time to date \n",
    "    inc_df['opened_at'] = pd.to_datetime(inc_df['opened_at'])\n",
    "    inc_df['opened_at_formatted'] = inc_df['opened_at'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Drop incidents that cannot be tied to a machine\n",
    "    inc_df = inc_df[inc_df['configuration_item'].notnull()]\n",
    "\n",
    "    # Investigate duplicate incidents \n",
    "    grouped_counts = inc_df.groupby(['opened_at_formatted', 'configuration_item']).size()\n",
    "    num_inc_machine_dups = grouped_counts[grouped_counts > 1].sum()\n",
    "    print(\"Number of incidents that are apart of a duplicate machine-day combination: \", num_inc_machine_dups)\n",
    "\n",
    "    # identify duplicates \n",
    "    grouped_counts = grouped_counts.reset_index().rename(columns={0:'count'})\n",
    "    grouped_counts['dup'] = grouped_counts['count'].apply(lambda x: '1' if x > 1 else 0)\n",
    "    grouped_counts['group_index'] = list(range(len(grouped_counts)))\n",
    "    inc_df2 = pd.merge(inc_df, grouped_counts, how='left', left_on=['opened_at_formatted', 'configuration_item'],\n",
    "                        right_on=['opened_at_formatted', 'configuration_item'])\n",
    "    \n",
    "    if return_dups_only:\n",
    "        \n",
    "        # Select only relevant columns for analysis \n",
    "        select_columns = ['configuration_item', 'opened_at_formatted', 'number',\n",
    "                            'category', 'subcategory', 'short_description_NER',\n",
    "                            'u_cause_code', 'calling_user_id', 'opened_at', \n",
    "                            'closed_at', 'severity', 'urgency', 'count', 'dup', 'group_index']\n",
    "        inc_df2 = inc_df2[select_columns]\n",
    "\n",
    "        # Output a dataframe showing only duplicates on configuration_item, opened_at groupings\n",
    "        inc_df2 = inc_df2.query(\"\"\"`dup` == '1'\"\"\")\n",
    "        inc_df2 = inc_df2.sort_values(['count', 'configuration_item', 'opened_at_formatted'], ascending=[False, True, True])\n",
    "    \n",
    "    return inc_df2\n",
    "\n",
    "# Get the data and view in dtale;\n",
    "inc_df2 = identify_duplicate_machine_groups(inc_df)\n",
    "# dtale.show(inc_df2).open_browser()\n",
    "\n",
    "# or, optionally export to excel and view in a spreadsheet software\n",
    "# inc_df2.to_excel('do_not_commit/Datasets/duplicate_inc_explore.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations for multiple incidents filed on the same day\n",
    "* In many instances, the multiple incidents appear to be addressing the same problem and introducing redundancy in the dataset. For example, machine name `CHI-L-U31514` on `2023-06-15` has 3 incidents all referring to a missing macabacus add-in for Excel.\n",
    "* It appears that a user will call the service desk about one issue but afterwards request help with another, potentially unrelated issue. For example, machine name `HIB-L-U29727` on `2023-04-18` had issues with excel and one note not syncing, then requested help with Zoom install. \n",
    "* Some of these INC appear to be related to new computers that are experiencing multiple issues. For example, machine name `ENDPOINTIH3UHZ2` on `2023-03-13` was a new laptop but had audio, display, and OneNote syncing issues (hence, multiple INC). \n",
    "\n",
    "### Options for path forward on duplicates\n",
    "1. Given our time constraints, it may be best to simply drop the duplicates on `configuration_item` and `opened_at_formatted`, so we only have one incident per machine per day. The disadvantage with this approach is that we lose detailed information about multiple issues on a machine. \n",
    "2. We could combine the textual data for the multiple incidents occurring on the same machine on the same day, such that if there were three incidents, they would all be mapped to one incident. The disadvantage of this approach is that it may not be as easy to sort or search for incidents on `category` or `short_description`. \n",
    "\n",
    "I'm going to proceed with option 2 and try to combine the incidents into one record for a given group. \n",
    "\n",
    "### Process for mapping multiple INC to one INC:\n",
    "1. For each group, do the following:\n",
    "    * Combine `short_description_NER` text into one string, but separate multiple INC `short_description` with a semicolon `;`\n",
    "    * For groups containing more than 2 INC, grab the most common `category`, `subcategory`, and `u_cause_code`, otherwise grab the first category, subcategory, and u_cause_code.\n",
    "    * Assign all other values with the attributes associated with the minimum `opened_at` time. \n",
    "    * Assign the `closed_at` time with the maximum date for the group. \n",
    "2. Drop the duplicate group-related records from the main INC dataset.\n",
    "3. Combine the output processed INC records with the main INC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_inc_records(in_df):\n",
    "\n",
    "    # If there are no duplicates, just return the row\n",
    "    if len(in_df) <= 1:\n",
    "        out_result = {col: in_df[col].iloc[0] for col in in_df.columns}\n",
    "\n",
    "    # Process if there are duplicates\n",
    "    else:\n",
    "        \n",
    "        # define output dictionary\n",
    "        return_dict= {}\n",
    "\n",
    "        # Get unique short_descriptions and merge them\n",
    "        short_des = '; '.join(list(in_df['short_description_NER'].unique()))\n",
    "        return_dict['short_description_NER'] = short_des\n",
    "\n",
    "        # get the most frequent occurring value for categories and cause code\n",
    "        for col in ['category', 'subcategory', 'u_cause_code']:\n",
    "\n",
    "            out_dict = in_df.groupby(col).size().to_dict()\n",
    "            sorted_dict = {k: v for k, v in sorted(out_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "            out_val = list(sorted_dict.keys())[0]\n",
    "            return_dict[col] = out_val\n",
    "    \n",
    "        # Get max close time\n",
    "        return_dict['closed_at'] = in_df['closed_at'].max()\n",
    "\n",
    "        # Get record associated with the min open time\n",
    "        out_df = in_df.loc[in_df['opened_at'].idxmin()]\n",
    "\n",
    "        # assign output result\n",
    "        out_result = {}\n",
    "        for val in out_df.index:\n",
    "\n",
    "            if val not in return_dict.keys():\n",
    "                out_result[val] = out_df[val]\n",
    "            else:\n",
    "                out_result[val] = return_dict[val]\n",
    "        \n",
    "    return pd.Series(out_result, index=list(in_df.columns))\n",
    "\n",
    "\n",
    "def process_dup_inc(in_df):\n",
    "\n",
    "    # Get the duplicate groups \n",
    "    dup_inc = identify_duplicate_machine_groups(inc_df, return_dups_only=False)\n",
    "\n",
    "    # change closed_at and opened_at to type datetime\n",
    "    for val in ['opened_at', 'closed_at']:\n",
    "        dup_inc[val] = pd.to_datetime(dup_inc[val])\n",
    "\n",
    "    # Group records and process duplicates\n",
    "    out_df = dup_inc.groupby('group_index').apply(merge_inc_records)\n",
    "\n",
    "    return out_df\n",
    "    \n",
    "processed_df = process_dup_inc(inc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spot check on records that were merged \n",
    "processed_df.rename(columns={'group_index':'group_index_orig'}, inplace=True)\n",
    "#dtale.show(processed_df).open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original dataframe length for records that had a machine name: ', len(inc_df[inc_df['configuration_item'].notnull()]))\n",
    "print('newly processed dataframe length: ', len(processed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document dependencies in a jupyter notebook\n",
    "\n",
    "# Dependencies for this notebook\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
