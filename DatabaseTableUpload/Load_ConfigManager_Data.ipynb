{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Config Manager data to the Azure Database\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "-  Python dependencies:\n",
    "    - pandas : 2.0.0,\n",
    "    - json   : 2.0.9,\n",
    "    - pyarrow: 12.0.1,\n",
    "    - pyodbc : 4.0.39,\n",
    "    - azure  : 5.0.0\n",
    "- `fun.json` for access credentials\n",
    "- `/create_sql` folder for SQL code to create tables\n",
    "\n",
    "I assigned us all an equal number of tables to load in the Historic Config Manager Summary excel document in our shared google drive. The names of the parquet files in Azure Blob Storage are also included there.\n",
    "See the link below: https://docs.google.com/spreadsheets/d/1GTahzraMPXh9RTNQHnqzBrZaee4qBTkjZV6v58WjhfM/edit?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "### Section 1 - Workflow Instructions \n",
    "\n",
    "1. Assemble prerequisites. Ensure the python dependencies listed above are installed in your working environment. Make a subdirectory in your working directory called `/assets` and place the `fun.json` file there.\n",
    "\n",
    "2. Download the config manager parquet files from Azure Blob Storage. Use the code in section 2. Credentials needed to access the storage container are provided in `fun.json`. \n",
    "\n",
    "3. Create the SQL table in the Azure database for each table. Connect to the Azure SQL Database using DBeaver or another preferred database development environment. Run the SQL code for the corresponding table in `/assets/create_sql` to create each config manager database table. \n",
    "\n",
    "4. Load the SQL table with data from the parquet file. Use the code in section 3. Feel free to optimize this code to make the load process faster!\n",
    "\n",
    "5. Repeat steps 2 through 4 for all Config manager tables that were assigned.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Section 2 - Download parquet files from Azure Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T15:26:42.521962444Z",
     "start_time": "2023-08-01T15:26:42.480416185Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pyodbc\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T15:26:44.246891638Z",
     "start_time": "2023-08-01T15:26:44.225044856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in credentials\n",
    "with open('./assets/fun.json') as f:\n",
    "    cred = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T15:30:23.613697048Z",
     "start_time": "2023-08-01T15:30:22.635160385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 0.9568779468536377 seconds to download boot_events_parsed_features.pq\n"
     ]
    }
   ],
   "source": [
    "def download_blob(blob_name, local_file_name):\n",
    "    '''\n",
    "    Downloads the parquet file from the blob container.\n",
    "    '''\n",
    "\n",
    "    # Declare variables\n",
    "    STORAGEACCOUNTURL= cred['in_the_sun']\n",
    "    STORAGEACCOUNTKEY= cred['fun']\n",
    "    LOCALFILENAME= local_file_name\n",
    "    CONTAINERNAME= 'configmanagertest1'\n",
    "    BLOBNAME= blob_name\n",
    "\n",
    "    # Download from blob\n",
    "    t1=time.time()\n",
    "    blob_service_client_instance = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY)\n",
    "    blob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\n",
    "    with open(LOCALFILENAME, \"wb\") as my_blob:\n",
    "        blob_data = blob_client_instance.download_blob()\n",
    "        blob_data.readinto(my_blob)\n",
    "    t2=time.time()\n",
    "    print((\"It takes %s seconds to download \"+BLOBNAME) % (t2 - t1))\n",
    "\n",
    "# Call the function\n",
    "download_blob(blob_name='', local_file_name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 3 (optional) - Upload files to Azure Storage."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def upload_blob_file(\n",
    "    filepath:str,\n",
    "    filename:str,\n",
    "    blob_service_client:BlobServiceClient,\n",
    "    container_name:str):\n",
    "    '''\n",
    "    Given a filepath and filename, uploads this file to Azure blob storage.\n",
    "\n",
    "    :param filepath: The directory where the file exists, e.g. /home/user/...\n",
    "    :param filename: The name of the file to upload\n",
    "    :param blob_service_client: Azure BlobServiceClient() object. It requires 'account_url'=cred['in_the_sun'] and 'credential'=cred['fun'].\n",
    "    :param container_name: The container where the file will exist.\n",
    "    :return: A void function that uploads the file to Azure blob storage.\n",
    "    '''\n",
    "\n",
    "    container_client = blob_service_client.get_container_client(container=container_name)\n",
    "    with open(file=os.path.join(filepath, filename), mode=\"rb\") as data:\n",
    "        blob_client = container_client.upload_blob(name=filename, data=data, overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T15:28:55.095705918Z",
     "start_time": "2023-08-01T15:28:55.037636795Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "upload_blob_file(\n",
    "    filepath='',\n",
    "    filename='',\n",
    "    blob_service_client=BlobServiceClient(account_url=cred['in_the_sun'], credential=cred['fun']),\n",
    "    container_name='configmanagertest1'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T15:28:59.226478628Z",
     "start_time": "2023-08-01T15:28:55.475427979Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:42:51.544984298Z",
     "start_time": "2023-07-10T17:42:51.302619456Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in parquet file as DataFrame.\n",
    "df = pd.read_parquet('./assets/UserDisc.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace empty strings with None.\n",
    "df = df.applymap(lambda x: None if x == \"\" else x)\n",
    "count = (df.eq(\"\")).sum().sum()\n",
    "print(\"Number of empty strings:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace any np.inf values with np.nan.\n",
    "df.replace({np.inf: np.nan, -np.inf: np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pq = pq.where(pd.notnull(pq), None)\n",
    "# pq.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert float64 columns to int.\n",
    "float_columns = df.select_dtypes(include=['float64']).columns\n",
    "df[float_columns] = df[float_columns].fillna(0)\n",
    "df[float_columns] = df[float_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save out parquet file.\n",
    "df.to_parquet('./assets/BatteryCorrected.parquet', engine='pyarrow')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 - Load data to the Azure SQL table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "#### Make the database table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Connect to database.\n",
    "1. Connect using SQL editor of choice (DBeaver, DataGrip).\n",
    "2. Select Azure SQL using built-in connection type.\n",
    "3. In pop-up dialog window, enter connection credentials using values from `fun.json` to authenticate.\n",
    "4. Connect to Azure DB.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Add table.\n",
    "1. Find `Tables` subdirectory under `dbo` schema.\n",
    "2. Right-click to open context window. Highlight \"Add...\". Click \"New table\".\n",
    "3. Dialog window appears. Provide SQL template code from `create_sql` directory for the corresponding table.\n",
    "    - Change table name in dialog window to `Persist.Table_Name`.\n",
    "    - You may need to edit the SQL to successfully create the correct table.\n",
    "    - Remove `IDENTITY(1,1)`\n",
    "    - Remove `ON [filegroup]` statement from `WITH` near bottom.\n",
    "    - Change table name to: `dbo.[Persist.Table_Name]`\n",
    "4. Once the table is created, run the below function to push your data to the table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "#### Upload the DataFrame as a parquet file to the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conn string\n",
    "conn_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={cred['server']};DATABASE={cred['db']};UID={cred['uid']};PWD={cred['pwd']}\"\n",
    "# Establish a connection to the SQL Server database\n",
    "\n",
    "conn = pyodbc.connect(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_table_v2(conn_obj, table_name, pq_file_name, stop_at):\n",
    "    '''\n",
    "    Loads a full table of data into the table_name of the\n",
    "    connect database in the conn_obj. Uses chunking for an\n",
    "    efficient load.\n",
    "    '''\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn_obj.cursor()\n",
    "\n",
    "    # Read in the parquet file\n",
    "    parquet_data = pq.ParquetFile(pq_file_name) # RJ 07/08/2023: Subset .pq here.\n",
    "\n",
    "    # Get the field names from the parquet file schema\n",
    "    pq_field_names = parquet_data.schema_arrow.names\n",
    "    col_tup_str_curr = '(' + ','.join(pq_field_names) +')'\n",
    "\n",
    "    # Get the number of (?) to duplicate in the insert query\n",
    "    num_qs_rep = len(col_tup_str_curr.split(','))\n",
    "\n",
    "    # Make the values string\n",
    "    values_ques_str = ','.join(tuple(['?']*num_qs_rep))\n",
    "\n",
    "    # Define the SQL insert statement\n",
    "    insert_query = \"INSERT INTO \" + table_name + \" \" + col_tup_str_curr + \" VALUES (\" + values_ques_str + \")\"\n",
    "\n",
    "    # Read the Parquet file in chunks\n",
    "    chunk_size = 10000\n",
    "    num_rows = parquet_data.metadata.num_rows\n",
    "    num_chunks = num_rows // chunk_size + 1\n",
    "\n",
    "    # Track the rows loaded during the insert\n",
    "    rows_loaded = 0\n",
    "    track_to_100k = 0\n",
    "\n",
    "    # Process and insert the data in chunks\n",
    "    batch_number = 0\n",
    "    batch_stop = 100 # Break at 1,000,000 rows.\n",
    "    for batch in parquet_data.iter_batches(batch_size=chunk_size):\n",
    "\n",
    "        # Stop after n rows\n",
    "        if rows_loaded == stop_at:\n",
    "            break\n",
    "\n",
    "        # Read the chunk of data from Parquet.\n",
    "        chunk = batch.to_pandas()\n",
    "        # Convert the chunk to a list of tuples.\n",
    "        records = [tuple(row) for row in chunk.itertuples(index=False)]\n",
    "        # Execute the INSERT statement with executemany().\n",
    "        cursor.executemany(insert_query, records)\n",
    "        conn_obj.commit()\n",
    "\n",
    "        # Notify when 100K rows are loaded\n",
    "        rows_loaded = rows_loaded + len(chunk)\n",
    "        track_to_100k = len(chunk) + track_to_100k\n",
    "        if track_to_100k >= 100000:\n",
    "            print(f\"{rows_loaded} number of rows loaded.\")\n",
    "            track_to_100k = 0\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "\n",
    "# Call the function\n",
    "load_table_v2(conn_obj=conn, table_name='dbo.[Persist.BATTERY_DATA]', pq_file_name='assets/BatteryCorrected.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document dependencies in a jupyter notebook\n",
    "# Dependencies for this notebook\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
