{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Addins, Programs, and Updates\n",
    "This notebook is intended to engineer the features from `OFFICE_ADDIN_DATA`, `Add_Remove_Programs`, and update events from `EventRawResultItem`.\n",
    "\n",
    "### Engineer update event features\n",
    "Here I will create 3 features:\n",
    "1.\tNumber of updates installed \n",
    "2.\tNumber of Windows 10 updates installed\n",
    "3.\tNumber of office updates installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dtale\n",
    "\n",
    "# Set the notebook to display all columns of a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def handle_none_values(in_val):\n",
    "\n",
    "    if type(in_val) == type(None) or in_val == 'None':\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return in_val\n",
    "    \n",
    "def read_json_fill_attr(jsonfile, in_df, attr):\n",
    "\n",
    "    attr_df = pd.read_json(jsonfile, orient='index')\n",
    "    in_dict = attr_df.reset_index().set_index('id').to_dict(orient='index')\n",
    "    in_df[attr] = in_df[attr].apply(lambda x: in_dict[x]['index'])\n",
    "\n",
    "    return in_df\n",
    "\n",
    "def read_update_data(infile):\n",
    "\n",
    "    # Read in INC and category df\n",
    "    inc_df = pd.read_parquet(infile)\n",
    "\n",
    "    # Filter out erroneous Nones in the data \n",
    "    for col in inc_df.columns:\n",
    "        inc_df[col] = inc_df[col].apply(lambda x: handle_none_values(x))\n",
    "\n",
    "    out_inc = read_json_fill_attr('assets/updateTitle.json', inc_df, 'updateTitle')\n",
    "\n",
    "    return out_inc\n",
    "\n",
    "# Get incident data \n",
    "df = read_update_data('assets/update_events.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_counts(in_dat):\n",
    "\n",
    "    output_dict = {}\n",
    "\n",
    "    # Get total number of updates\n",
    "    output_dict['num_updates'] = len(in_dat)\n",
    "\n",
    "    # Get the number of windows os updates\n",
    "    output_dict['num_windows_64_os_updates'] = in_dat['win_64_os_update'].sum()\n",
    "\n",
    "    # Get the number of office updates\n",
    "    output_dict['num_office_updates'] = in_dat['office_update'].sum()\n",
    "\n",
    "    # Return a series for the group \n",
    "    out_series = pd.Series(output_dict, index=list(output_dict.keys()))\n",
    "\n",
    "    return out_series\n",
    " \n",
    "def get_update_features(in_df):\n",
    "\n",
    "    # lower case for updateTitle \n",
    "    in_df['updateTitle'] = in_df['updateTitle'].str.lower()\n",
    "\n",
    "    # Change created system time to type datetime\n",
    "    in_df['TimeCreatedSystemTime'] = pd.to_datetime(in_df['TimeCreatedSystemTime'])\n",
    "\n",
    "    # Add created date \n",
    "    in_df['created_date'] = in_df['TimeCreatedSystemTime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create identifier for x64 based Windows OS updates \n",
    "    in_df['win_64_os_update'] = in_df['updateTitle'].apply(lambda x: 1 if ('cumulative update for' in x) or ('windows 10' in x) else 0)\n",
    "\n",
    "    # Create identifier for office updates\n",
    "    in_df['office_update'] = in_df['updateTitle'].apply(lambda x: 1 if 'office' in x else 0)\n",
    "\n",
    "    # Group by and get results \n",
    "    out_gb = in_df.groupby(['ClientItemKey', 'created_date']).apply(get_update_counts).reset_index()\n",
    "\n",
    "    return in_df, out_gb\n",
    "\n",
    "# Get features\n",
    "processd_df, grouped_df = get_update_features(df)\n",
    "\n",
    "# Confirm results are expected\n",
    "# dtale.show(grouped_df).open_browser()\n",
    "\n",
    "# Save result to parquet\n",
    "grouped_df.to_parquet('assets/update_summary_features.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer Addin Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def read_addin_data(addin_data_directory, chunk_size):\n",
    "\n",
    "    # Get the files in the directory\n",
    "    files = os.listdir(addin_data_directory)\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "\n",
    "            filepath = os.path.join(addin_data_directory, file)\n",
    "            \n",
    "            # Read in the parquet file\n",
    "            parquet_data = pq.ParquetFile(filepath)\n",
    "\n",
    "            # Process the data in chunks\n",
    "            for batch in parquet_data.iter_batches(chunk_size):\n",
    "\n",
    "                # Read the chunk of data from Parquet\n",
    "                chunk = batch.to_pandas()\n",
    "\n",
    "                # Fill friendly name, product name, and company name with values\n",
    "                friendlyname = os.path.join(addin_data_directory, 'FriendlyName.json')\n",
    "                companyname = os.path.join(addin_data_directory, 'CompanyName.json')\n",
    "                productname = os.path.join(addin_data_directory, 'ProductName.json')\n",
    "                chunk = read_json_fill_attr(friendlyname, chunk, 'FriendlyName00')\n",
    "                chunk = read_json_fill_attr(companyname, chunk, 'CompanyName00')\n",
    "                chunk = read_json_fill_attr(productname, chunk, 'ProductName00')\n",
    "\n",
    "                yield chunk\n",
    "\n",
    "\n",
    "dfs = read_addin_data('assets/office_addin_data', 1000000)\n",
    "\n",
    "addin_df = next(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = addin_df.drop('rowversion', axis=1)\n",
    "dtale.show(test_df).open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def identify_addin(in_dat, attr_name, re_pat):\n",
    "\n",
    "    in_dat[attr_name] = in_dat['FriendlyName00'].apply(lambda x: 1 if re.search(re_pat, x) else 0)\n",
    "\n",
    "    return in_dat\n",
    "\n",
    "def create_addin_features(in_df):\n",
    "\n",
    "    # Change effective date to type date\n",
    "    in_df['RWB_EFFECTIVE_DATE'] = pd.to_datetime(in_df['RWB_EFFECTIVE_DATE']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create identifier for CAP IQ \n",
    "    cap_pat = 'Cap IQ|Capital IQ|cap iq|capital iq'\n",
    "    in_df = identify_addin(in_df, 'has_cap_iq', cap_pat)\n",
    "\n",
    "    # Create identifier for FactSet \n",
    "    cap_pat = 'FactSet|factset'\n",
    "    in_df = identify_addin(in_df, 'has_factset', cap_pat)\n",
    "\n",
    "    # Create identifier for BlueMatrix, Bloomberg, and acrobat\n",
    "    in_df['has_bluematrix'] = in_df['CompanyName00'].apply(lambda x: 1 if x == 'BlueMatrix I LLC' else 0)\n",
    "    in_df['has_bloomberg'] = in_df['CompanyName00'].apply(lambda x: 1 if x == 'Bloomberg LP' else 0)\n",
    "    in_df['has_acrobat'] = in_df['CompanyName00'].apply(lambda x: 1 if x == 'Adobe Systems Incorporated' else 0)\n",
    "\n",
    "    out_gb = in_df.groupby(['MachineID', 'RWB_EFFECTIVE_DATE', 'Architecture00']).apply(lambda x: pd.Series({\"num_addins\": len(x['Id00'].unique())}, index=[\"num_addins\"]))\n",
    "\n",
    "    return in_df, out_gb\n",
    "\n",
    "processed_df, grouped_df = create_addin_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle to view in dtale.\n",
    "# dtale.show(processed_df).open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
